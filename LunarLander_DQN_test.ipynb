{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "hyynyQeNUrq4",
        "colab_type": "code",
        "outputId": "f1e5540a-9ba8-4bae-acd9-a3e991a93a86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install gym > /dev/null\n",
        "!apt-get install python-opengl -y > /dev/null\n",
        "!apt install xvfb -y > /dev/null\n",
        "!pip install pyvirtualdisplay > /dev/null\n",
        "!pip install piglet > /dev/null\n",
        "\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl  > /dev/null\n",
        "!pip3 install torchvision > /dev/null\n",
        "!pip install box2d-py > /dev/null\n",
        "!pip install gym[Box_2D] > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\u001b[33m  gym 0.10.9 does not provide the extra 'box_2d'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JA9FofJDKTAw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Init\n",
        "========="
      ]
    },
    {
      "metadata": {
        "id": "-E4ut7KKK8fF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import sys\n",
        "import pickle\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1\n",
        "      \n",
        "#from collections import namedtuple\n",
        "#from itertools import count\n",
        "#from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "#import torchvision.transforms as T\n",
        "#from torch.autograd import autograd\n",
        "#import torch.nn.utils as utils\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyJJIjHerMnb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_global_seeds(i):\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "        pass\n",
        "    else:\n",
        "        torch.manual_seed(i)\n",
        "    np.random.seed(i)\n",
        "    random.seed(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wK32FKuVtQD3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LinearSchedule(object):\n",
        "    def __init__(self, schedule_timesteps, final_p, initial_p=1.0):\n",
        "        \"\"\"Linear interpolation between initial_p and final_p over\n",
        "        schedule_timesteps. After this many timesteps pass final_p is\n",
        "        returned.\n",
        "        Parameters\n",
        "        ----------\n",
        "        schedule_timesteps: int\n",
        "            Number of timesteps for which to linearly anneal initial_p\n",
        "            to final_p\n",
        "        initial_p: float\n",
        "            initial output value\n",
        "        final_p: float\n",
        "            final output value\n",
        "        \"\"\"\n",
        "        self.schedule_timesteps = schedule_timesteps\n",
        "        self.final_p            = final_p\n",
        "        self.initial_p          = initial_p\n",
        "\n",
        "    def value(self, t):\n",
        "        \"\"\"See Schedule.value\"\"\"\n",
        "        fraction  = min(float(t) / self.schedule_timesteps, 1.0)\n",
        "        return self.initial_p + fraction * (self.final_p - self.initial_p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g4Dyjbz-ucOM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_n_unique(sampling_f, n):\n",
        "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
        "    comparable objects, sample n such unique objects.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    while len(res) < n:\n",
        "        candidate = sampling_f()\n",
        "        if candidate not in res:\n",
        "            res.append(candidate)\n",
        "    return res\n",
        "  \n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, frame_history_len):\n",
        "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
        "        The sepecific memory optimizations use here are:\n",
        "            - only store each frame once rather than k times\n",
        "              even if every observation normally consists of k last frames\n",
        "            - store frames as np.uint8 (actually it is most time-performance\n",
        "              to cast them back to float32 on GPU to minimize memory transfer\n",
        "              time)\n",
        "            - store frame_t and frame_(t+1) in the same buffer.\n",
        "        For the typical use case in Atari Deep RL buffer with 1M frames the total\n",
        "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
        "        Warning! Assumes that returning frame of zeros at the beginning\n",
        "        of the episode, when there is less frames than `frame_history_len`,\n",
        "        is acceptable.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        frame_history_len: int\n",
        "            Number of memories to be retried for each observation.\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.frame_history_len = frame_history_len\n",
        "\n",
        "        self.next_idx      = 0\n",
        "        self.num_in_buffer = 0\n",
        "\n",
        "        self.obs      = None\n",
        "        self.action   = None\n",
        "        self.reward   = None\n",
        "        self.done     = None\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
        "        return batch_size + 1 <= self.num_in_buffer\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obs_batch      = np.concatenate([self._encode_observation(idx)[np.newaxis, :] for idx in idxes], 0)\n",
        "        act_batch      = self.action[idxes]\n",
        "        rew_batch      = self.reward[idxes]\n",
        "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[np.newaxis, :] for idx in idxes], 0)\n",
        "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
        "\n",
        "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample `batch_size` different transitions.\n",
        "        i-th sample transition is the following:\n",
        "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
        "        after which reward `rew_batch[i]` was received and subsequent\n",
        "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
        "        was done which is represented by `done_mask[i]` which is equal\n",
        "        to 1 if episode has ended as a result of that action.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            Array of shape\n",
        "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
        "            and dtype np.uint8\n",
        "        act_batch: np.array\n",
        "            Array of shape (batch_size,) and dtype np.int32\n",
        "        rew_batch: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "        next_obs_batch: np.array\n",
        "            Array of shape\n",
        "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
        "            and dtype np.uint8\n",
        "        done_mask: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "        \"\"\"\n",
        "        assert self.can_sample(batch_size)\n",
        "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
        "        return self._encode_sample(idxes)\n",
        "\n",
        "    def encode_recent_observation(self):\n",
        "        \"\"\"Return the most recent `frame_history_len` frames.\n",
        "        Returns\n",
        "        -------\n",
        "        observation: np.array\n",
        "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
        "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
        "            encodes frame at time `t - frame_history_len + i`\n",
        "        \"\"\"\n",
        "        assert self.num_in_buffer > 0\n",
        "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
        "\n",
        "    def _encode_observation(self, idx):\n",
        "        end_idx   = idx + 1 # make noninclusive\n",
        "        start_idx = end_idx - self.frame_history_len\n",
        "        # this checks if we are using low-dimensional observations, such as RAM\n",
        "        # state, in which case we just directly return the latest RAM.\n",
        "        if len(self.obs.shape) == 2:\n",
        "            return self.obs[end_idx-1]\n",
        "        # if there weren't enough frames ever in the buffer for context\n",
        "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
        "            start_idx = 0\n",
        "        for idx in range(start_idx, end_idx - 1):\n",
        "            if self.done[idx % self.size]:\n",
        "                start_idx = idx + 1\n",
        "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
        "        # if zero padding is needed for missing context\n",
        "        # or we are on the boundry of the buffer\n",
        "        if start_idx < 0 or missing_context > 0:\n",
        "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
        "            for idx in range(start_idx, end_idx):\n",
        "                frames.append(self.obs[idx % self.size])\n",
        "            return np.concatenate(frames, 0)\n",
        "        else:\n",
        "            # this optimization has potential to saves about 30% compute time \\o/\n",
        "            img_h, img_w = self.obs.shape[2], self.obs.shape[3]\n",
        "            return self.obs[start_idx:end_idx].reshape(-1, img_h, img_w)\n",
        "\n",
        "    def store_frame(self, frame):\n",
        "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
        "        old frames if necessary.\n",
        "        Parameters\n",
        "        ----------\n",
        "        frame: np.array\n",
        "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
        "            and the frame will transpose to shape (img_h, img_w, img_c) to be stored\n",
        "        Returns\n",
        "        -------\n",
        "        idx: int\n",
        "            Index at which the frame is stored. To be used for `store_effect` later.\n",
        "        \"\"\"\n",
        "        # make sure we are not using low-dimensional observations, such as RAM\n",
        "        if len(frame.shape) > 1:\n",
        "            # transpose image frame into (img_c, img_h, img_w)\n",
        "            frame = frame.transpose(2, 0, 1)\n",
        "\n",
        "        if self.obs is None:\n",
        "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n",
        "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
        "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
        "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
        "\n",
        "        self.obs[self.next_idx] = frame\n",
        "\n",
        "        ret = self.next_idx\n",
        "        self.next_idx = (self.next_idx + 1) % self.size\n",
        "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def store_effect(self, idx, action, reward, done):\n",
        "        \"\"\"Store effects of action taken after obeserving frame stored\n",
        "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
        "        up into two functions is so that one can call `encode_recent_observation`\n",
        "        in between.\n",
        "        Paramters\n",
        "        ---------\n",
        "        idx: int\n",
        "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
        "        action: int\n",
        "            Action that was performed upon observing this frame.\n",
        "        reward: float\n",
        "            Reward that was received when the actions was performed.\n",
        "        done: bool\n",
        "            True if episode was finished after performing that action.\n",
        "        \"\"\"\n",
        "        self.action[idx] = action\n",
        "        self.reward[idx] = reward\n",
        "        self.done[idx]   = done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2UgwE-krmJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def wrap_deepmind(env):\n",
        "    #logging.warning('Watch out!')\n",
        "    #logging.warning(env.spec.id)\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    #if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "    #    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ClippedRewardsWrapper(env)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BhmwiG4ZKWMJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_env(name, seed):\n",
        "\n",
        "    env = gym.make(name)\n",
        "    \n",
        "    env.reset()\n",
        "    \n",
        "    plt.imshow(env.render(mode='rgb_array'))\n",
        "    print(env.render(mode='rgb_array').shape)\n",
        "    print(\"Observation space:\", env.observation_space)\n",
        "    print(\"Action space:\", env.action_space)\n",
        "\n",
        "    set_global_seeds(seed)\n",
        "    env.seed(seed)\n",
        "\n",
        "    expt_dir = 'tmp/gym-results'\n",
        "    env = wrappers.Monitor(env, expt_dir, force=True)\n",
        "    \n",
        "    #env = wrap_deepmind(env)\n",
        "\n",
        "    return env\n",
        "  \n",
        "def get_wrapper_by_name(env, classname):\n",
        "    currentenv = env\n",
        "    while True:\n",
        "        if classname in currentenv.__class__.__name__:\n",
        "            return currentenv\n",
        "        elif isinstance(env, gym.Wrapper):\n",
        "            currentenv = currentenv.env\n",
        "        else:\n",
        "            raise ValueError(\"Couldn't find wrapper named %s\"%classname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3UbQ9fstpPm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_channels=4, num_actions=18):\n",
        "        \"\"\"\n",
        "        Initialize a deep Q-learning network as described in\n",
        "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
        "        Arguments:\n",
        "            in_channels: number of channel of input.\n",
        "                i.e The number of most recent frames stacked together as describe in the paper\n",
        "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(46 * 71 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, num_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VSbAUokAsqSI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available\n",
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "class Variable(autograd.Variable):\n",
        "    def __init__(self, data, *args, **kwargs):\n",
        "        if USE_CUDA:\n",
        "            data = data.cuda()\n",
        "        super(Variable, self).__init__(data, *args, **kwargs)\n",
        "\n",
        "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\"])\n",
        "\n",
        "Statistic = {\n",
        "    \"mean_episode_rewards\": [],\n",
        "    \"best_mean_episode_rewards\": []\n",
        "}\n",
        "\n",
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "def dqn_learing(\n",
        "    env,\n",
        "    q_func,\n",
        "    optimizer_spec,\n",
        "    exploration,\n",
        "    stopping_criterion=None,\n",
        "    replay_buffer_size=1000000,\n",
        "    batch_size=32,\n",
        "    gamma=0.99,\n",
        "    learning_starts=50000,\n",
        "    learning_freq=4,\n",
        "    frame_history_len=4,\n",
        "    target_update_freq=10000\n",
        "    ):\n",
        "\n",
        "    \"\"\"Run Deep Q-learning algorithm.\n",
        "    You can specify your own convnet using q_func.\n",
        "    All schedules are w.r.t. total number of steps taken in the environment.\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.Env\n",
        "        gym environment to train on.\n",
        "    q_func: function\n",
        "        Model to use for computing the q function. It should accept the\n",
        "        following named arguments:\n",
        "            input_channel: int\n",
        "                number of channel of input.\n",
        "            num_actions: int\n",
        "                number of actions\n",
        "    optimizer_spec: OptimizerSpec\n",
        "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
        "        for the optimizer\n",
        "    exploration: Schedule (defined in utils.schedule)\n",
        "        schedule for probability of chosing random action.\n",
        "    stopping_criterion: (env) -> bool\n",
        "        should return true when it's ok for the RL algorithm to stop.\n",
        "        takes in env and the number of steps executed so far.\n",
        "    replay_buffer_size: int\n",
        "        How many memories to store in the replay buffer.\n",
        "    batch_size: int\n",
        "        How many transitions to sample each time experience is replayed.\n",
        "    gamma: float\n",
        "        Discount Factor\n",
        "    learning_starts: int\n",
        "        After how many environment steps to start replaying experiences\n",
        "    learning_freq: int\n",
        "        How many steps of environment to take between every experience replay\n",
        "    frame_history_len: int\n",
        "        How many past frames to include as input to the model.\n",
        "    target_update_freq: int\n",
        "        How many experience replay rounds (not steps!) to perform between\n",
        "        each update to the target Q network\n",
        "    \"\"\"\n",
        "    assert type(env.observation_space) == gym.spaces.Box\n",
        "    assert type(env.action_space)      == gym.spaces.Discrete\n",
        "\n",
        "    ###############\n",
        "    # BUILD MODEL #\n",
        "    ###############\n",
        "    \n",
        "    #print(env.observation_space.shape)\n",
        "\n",
        "    #if len(env.observation_space.shape) == 1:\n",
        "        # This means we are running on low-dimensional observations (e.g. RAM)\n",
        "    #    input_arg = env.observation_space.shape[0]\n",
        "    #else:\n",
        "        \n",
        "    img_h, img_w, img_c = env.render(mode='rgb_array').shape\n",
        "    input_arg = frame_history_len * img_c\n",
        "        \n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Construct an epilson greedy policy with given exploration schedule\n",
        "    def select_epilson_greedy_action(model, obs, t):\n",
        "        sample = random.random()\n",
        "        eps_threshold = exploration.value(t)\n",
        "        if sample > eps_threshold:\n",
        "            obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n",
        "            # Use volatile = True if variable is only used in inference mode, i.e. don’t save the history\n",
        "            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n",
        "        else:\n",
        "            return torch.IntTensor([random.randrange(num_actions)])\n",
        "\n",
        "    # Initialize target q function and q function\n",
        "    Q = q_func(input_arg, num_actions).type(dtype)\n",
        "    target_Q = q_func(input_arg, num_actions).type(dtype)\n",
        "\n",
        "    # Construct Q network optimizer function\n",
        "    optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
        "\n",
        "    # Construct the replay buffer\n",
        "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
        "\n",
        "    ###############\n",
        "    # RUN ENV     #\n",
        "    ###############\n",
        "    num_param_updates = 0\n",
        "    mean_episode_reward = -float('nan')\n",
        "    best_mean_episode_reward = -float('inf')\n",
        "    last_obs = env.reset()\n",
        "    last_obs = env.render(mode='rgb_array')\n",
        "    LOG_EVERY_N_STEPS = 100\n",
        "    \n",
        "    tprev = 0\n",
        "\n",
        "    for t in count():\n",
        "        ### Check stopping criterion\n",
        "        if stopping_criterion is not None and stopping_criterion(env):\n",
        "            print(\"stop\")\n",
        "            break\n",
        "\n",
        "        ### Step the env and store the transition\n",
        "        # Store lastest observation in replay memory and last_idx can be used to store action, reward, done\n",
        "        last_idx = replay_buffer.store_frame(last_obs)\n",
        "        # encode_recent_observation will take the latest observation\n",
        "        # that you pushed into the buffer and compute the corresponding\n",
        "        # input that should be given to a Q network by appending some\n",
        "        # previous frames.\n",
        "        recent_observations = replay_buffer.encode_recent_observation()\n",
        "\n",
        "        # Choose random action if not yet start learning\n",
        "        if t > learning_starts:\n",
        "            action = select_epilson_greedy_action(Q, recent_observations, t)[0].numpy()\n",
        "        else:\n",
        "            action = random.randrange(num_actions)\n",
        "            \n",
        "        # Advance one step\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        obs = env.render(mode='rgb_array')\n",
        "        #plt.imshow(obs)\n",
        "        # clip rewards between -1 and 1\n",
        "        reward = max(-1.0, min(reward, 1.0))        \n",
        "        # Store other info in replay memory\n",
        "        replay_buffer.store_effect(last_idx, action, reward, done)\n",
        "        # Resets the environment when reaching an episode boundary.\n",
        "        if done:\n",
        "            tprev = t\n",
        "            obs = env.reset()\n",
        "            obs = env.render(mode='rgb_array')\n",
        "        last_obs = obs\n",
        "\n",
        "        ### Perform experience replay and train the network.\n",
        "        # Note that this is only done if the replay buffer contains enough samples\n",
        "        # for us to learn something useful -- until then, the model will not be\n",
        "        # initialized and random actions should be taken\n",
        "        if (t > learning_starts and\n",
        "                t % learning_freq == 0 and\n",
        "                replay_buffer.can_sample(batch_size)):\n",
        "            # Use the replay buffer to sample a batch of transitions\n",
        "            # Note: done_mask[i] is 1 if the next state corresponds to the end of an episode,\n",
        "            # in which case there is no Q-value at the next state; at the end of an\n",
        "            # episode, only the current state reward contributes to the target\n",
        "            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
        "            # Convert numpy nd_array to torch variables for calculation\n",
        "            obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype) / 255.0)\n",
        "            act_batch = Variable(torch.from_numpy(act_batch).long())\n",
        "            rew_batch = Variable(torch.from_numpy(rew_batch))\n",
        "            next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype) / 255.0)\n",
        "            not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)\n",
        "\n",
        "            if USE_CUDA:\n",
        "                act_batch = act_batch.cuda()\n",
        "                rew_batch = rew_batch.cuda()\n",
        "\n",
        "            # Compute current Q value, q_func takes only state and output value for every state-action pair\n",
        "            # We choose Q based on action taken.\n",
        "            current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n",
        "            # Compute next Q value based on which action gives max Q values\n",
        "            # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
        "            next_max_q = target_Q(next_obs_batch).detach().max(1)[0]\n",
        "            next_Q_values = not_done_mask * next_max_q\n",
        "            # Compute the target of the current Q values\n",
        "            target_Q_values = rew_batch + (gamma * next_Q_values)\n",
        "            # Compute Bellman error\n",
        "            bellman_error = target_Q_values - current_Q_values\n",
        "            # clip the bellman error between [-1 , 1]\n",
        "            clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
        "            # Note: clipped_bellman_delta * -1 will be right gradient\n",
        "            d_error = clipped_bellman_error * -1.0\n",
        "            # Clear previous gradients before backward pass\n",
        "            optimizer.zero_grad()\n",
        "            # run backward pass\n",
        "            current_Q_values.backward(d_error.data.unsqueeze(-1)[0])\n",
        "\n",
        "            # Perfom the update\n",
        "            optimizer.step()\n",
        "            num_param_updates += 1\n",
        "\n",
        "            # Periodically update the target network by Q network to target Q network\n",
        "            if num_param_updates % target_update_freq == 0:\n",
        "                target_Q.load_state_dict(Q.state_dict())\n",
        "\n",
        "        ### 4. Log progress and keep track of statistics\n",
        "        episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
        "        if len(episode_rewards) > 0:\n",
        "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
        "        if len(episode_rewards) > 100:\n",
        "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
        "\n",
        "        Statistic[\"mean_episode_rewards\"].append(mean_episode_reward)\n",
        "        Statistic[\"best_mean_episode_rewards\"].append(best_mean_episode_reward)\n",
        "\n",
        "        if t % LOG_EVERY_N_STEPS == 0 and t > learning_starts:\n",
        "            print(\"Timestep %d\" % (t,))\n",
        "            print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
        "            print(\"best mean reward %f\" % best_mean_episode_reward)\n",
        "            print(\"episodes %d\" % len(episode_rewards))\n",
        "            print(\"exploration %f\" % exploration.value(t))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # Dump statistics to pickle\n",
        "            with open('statistics.pkl', 'wb') as f:\n",
        "                pickle.dump(Statistic, f)\n",
        "                print(\"Saved to %s\" % 'statistics.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2DmzvZdEsJce",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(env, num_timesteps):\n",
        "\n",
        "    def stopping_criterion(env):\n",
        "        # notice that here t is the number of steps of the wrapped env,\n",
        "        # which is different from the number of steps in the underlying env\n",
        "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
        "\n",
        "    optimizer_spec = OptimizerSpec(\n",
        "        constructor=optim.RMSprop,\n",
        "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
        "    )\n",
        "\n",
        "    exploration_schedule = LinearSchedule(100, 0.1)\n",
        "\n",
        "    dqn_learing(\n",
        "        env=env,\n",
        "        q_func=DQN,\n",
        "        optimizer_spec=optimizer_spec,\n",
        "        exploration=exploration_schedule,\n",
        "        stopping_criterion=stopping_criterion,\n",
        "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        gamma=GAMMA,\n",
        "        learning_starts=LEARNING_STARTS,\n",
        "        learning_freq=LEARNING_FREQ,\n",
        "        frame_history_len=FRAME_HISTORY_LEN,\n",
        "        target_update_freq=TARGER_UPDATE_FREQ,\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JFsQlXPxKSSX",
        "colab_type": "code",
        "outputId": "5a36e69c-873d-4409-a7bf-2fb1c036a72f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "GAMMA = 0.99\n",
        "REPLAY_BUFFER_SIZE = 10000\n",
        "LEARNING_STARTS = 500\n",
        "LEARNING_FREQ = 4\n",
        "FRAME_HISTORY_LEN = 4\n",
        "TARGER_UPDATE_FREQ = 100\n",
        "LEARNING_RATE = 0.00025\n",
        "ALPHA = 0.95\n",
        "EPS = 0.01\n",
        "\n",
        "seed = 50\n",
        "    \n",
        "env = get_env(\"LunarLander-v2\", seed)\n",
        "\n",
        "main(env, 100000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(400, 600, 3)\n",
            "Observation space: Box(8,)\n",
            "Action space: Discrete(4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Timestep 600\n",
            "mean reward (100 episodes) -180.496111\n",
            "best mean reward -inf\n",
            "episodes 6\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n",
            "Timestep 700\n",
            "mean reward (100 episodes) -246.229270\n",
            "best mean reward -inf\n",
            "episodes 7\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:101: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Timestep 800\n",
            "mean reward (100 episodes) -330.595427\n",
            "best mean reward -inf\n",
            "episodes 8\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n",
            "Timestep 900\n",
            "mean reward (100 episodes) -330.595427\n",
            "best mean reward -inf\n",
            "episodes 8\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n",
            "Timestep 1000\n",
            "mean reward (100 episodes) -399.734054\n",
            "best mean reward -inf\n",
            "episodes 10\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n",
            "Timestep 1100\n",
            "mean reward (100 episodes) -399.734054\n",
            "best mean reward -inf\n",
            "episodes 10\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n",
            "Timestep 1200\n",
            "mean reward (100 episodes) -449.559978\n",
            "best mean reward -inf\n",
            "episodes 11\n",
            "exploration 0.100000\n",
            "Saved to statistics.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}